
  <!DOCTYPE html>
<html lang="en">
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta charset="utf-8" />
    <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>4. Derivations | svGPFA 0.0.1 documentation</title>
    <link rel="stylesheet" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/theme.39dcd091ff2d4f33bf46.css" />
    <link rel="stylesheet" href="_static/proof.css" />
    <link rel="stylesheet" href="_static/sg_gallery.css" />
    <link rel="stylesheet" href="_static/sg_gallery-binder.css" />
    <link rel="stylesheet" href="_static/sg_gallery-dataframe.css" />
    <link rel="stylesheet" href="_static/sg_gallery-rendered-html.css" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="next" title="1. Parameters and their specification" href="params.html" />
    <link rel="prev" title="3. Implementation notes" href="implementationNotes.html" />
  </head>

  <body class="antialiased text-gray scroll-smooth">
    <div
      id="page"
      data-controller="sidebar search "
      data-action="keydown@window->search#focus "
      class="min-h-screen xl:h-screen flex flex-col xl:grid xl:grid-layout print:block print:h-auto"
    >
      <a class="block transition -translate-x-full focus:translate-x-0 opacity-0 focus:opacity-100 text-xl bg-white p-4 z-20 absolute top-0 left-0 h-14" href="#span-class-section-number-4-span-derivations" title="Skip navigation links">Skip to content</a>

      <header class="grid-area-header z-10 h-14 fixed w-full top-0 print:hidden">
        <div class="bg-gray-dark shadow-md flex items-center h-full xl:px-2 relative"><div class="flex items-center">
      <button
    data-action="sidebar#open"
    data-sidebar-target="hamburger"
    class="xl:hidden h-14 w-14 leading-14 text-gray-100 hover:bg-gray-700 hover:text-brand focus:outline-none focus:bg-gray-700 focus:text-brand">
  <span class="sr-only">Open navigation menu</span>
  <svg aria-hidden="true" class="fill-current h-8 w-8" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg>
</button><a class="hover:bg-gray-700 focus:bg-gray-700 focus:outline-none"
   href="index.html"
   title="Back to homepage"><span class="hidden lg:inline-block shrink-0 font-medium text-gray-100 mx-5 leading-14 tracking-wider">svGPFA 0.0.1 documentation</span>
</a></div><div class="flex justify-end items-center flex-1"><form
  id="searchbox"
  action="search.html"
  data-action="click->search#focusSearchInput"
  method="get"
  class="flex print:hidden justify-between items-center leading-14 md:ml-4 bg-gray-dark text-gray-300 focus-within:bg-gray-50 focus-within:text-gray-800 focus-within:absolute focus-within:inset-x-0 focus-within:top-0 md:focus-within:w-full md:focus-within:static z-10">

  <button
    class="text-inherit h-14 w-14"
    aria-label="Get search results"
    tabindex="-1"
  >
    <svg aria-hidden="true" class="fill-current stroke-current h-8 w-8" stroke-width="0.5" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M15.5 14h-.79l-.28-.27A6.471 6.471 0 0016 9.5 6.5 6.5 0 109.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z"/></svg>
  </button>

  <input
    name="q"
    id="search-input"
    data-search-target="searchInput"
    type="search"
    aria-label="Search the docs"
    placeholder="Search the docs"
    class="pr-2 bg-transparent text-inherit focus:outline-none w-0 md:w-auto focus:w-full transition-all duration-100"
  />
</form></div>
</div>
      </header>

      
<aside data-sidebar-target="sidebar" class="grid-area-sidebar h-full fixed pt-14 xl:relative inset-y-0 left-0 z-20 xl:z-0 print:hidden overflow-y-auto transition-all transform transform-gpu -translate-x-full opacity-0 duration-300 xl:translate-x-0 xl:opacity-100">
  <nav
  role="navigation"
  class="h-full overflow-y-auto bg-white text-gray-600 pt-8 flex flex-col"
>
  <div class="nav-toc flex-1 pl-6"><p class="caption" role="heading"><span class="caption-text">General notes:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="introduction.html">1. Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="highLevelInterface.html">2. High-level interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="lowLevelInterface.html">3. Low-level interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="auto_examples/index.html">4. Scripts for estimation and visualization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="auto_examples/plot_simulation.html">4.1. Simulated data and default params</a><ul>
<li class="toctree-l3"><a class="reference internal" href="auto_examples/plot_simulation.html#estimate-model">4.1.1. Estimate model</a></li>
<li class="toctree-l3"><a class="reference internal" href="auto_examples/plot_simulation.html#goodness-of-fit-analysis">4.1.2. Goodness-of-fit analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="auto_examples/plot_simulation.html#plotting">4.1.3. Plotting</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="auto_examples/plot_GPe.html">4.2. Basal ganglia recordings from a mouse performing a bandit task</a><ul>
<li class="toctree-l3"><a class="reference internal" href="auto_examples/plot_GPe.html#estimate-model">4.2.1. Estimate model</a></li>
<li class="toctree-l3"><a class="reference internal" href="auto_examples/plot_GPe.html#goodness-of-fit-analysis">4.2.2. Goodness-of-fit analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="auto_examples/plot_GPe.html#plotting">4.2.3. Plotting</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="auto_examples/plot_striatum_ibl.html">4.3. IBLâ€™s Recordings from the striatum of a mouse performing a visual discrimination task</a><ul>
<li class="toctree-l3"><a class="reference internal" href="auto_examples/plot_striatum_ibl.html#setup-environment">4.3.1. Setup environment</a></li>
<li class="toctree-l3"><a class="reference internal" href="auto_examples/plot_striatum_ibl.html#epoch">4.3.2. Epoch</a></li>
<li class="toctree-l3"><a class="reference internal" href="auto_examples/plot_striatum_ibl.html#get-parameters">4.3.3. Get parameters</a></li>
<li class="toctree-l3"><a class="reference internal" href="auto_examples/plot_striatum_ibl.html#estimate-svgpfa-model">4.3.4. Estimate svGPFA model</a></li>
<li class="toctree-l3"><a class="reference internal" href="auto_examples/plot_striatum_ibl.html#goodness-of-fit-analysis">4.3.5. Goodness-of-fit analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="auto_examples/plot_striatum_ibl.html#plotting">4.3.6. Plotting</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced notes:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="dataStructures.html">1. Data structures</a></li>
<li class="toctree-l1"><a class="reference internal" href="objectOrientedDesign.html">2. Object-oriented design</a></li>
<li class="toctree-l1"><a class="reference internal" href="implementationNotes.html">3. Implementation notes</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">4. Derivations</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#gpfa-model">4.1. GPFA model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#gpfa-with-inducing-points-model">4.2. GPFA with inducing points model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#svgpfa-variational-lower-bound">4.3. svGPFA variational lower bound</a></li>
<li class="toctree-l2"><a class="reference internal" href="#variational-distribution-of-h-nr-cdot">4.4. Variational distribution of <span class="math notranslate nohighlight">\(h_{nr}(\cdot)\)</span></a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Appendix:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="params.html">1. Parameters and their specification</a><ul>
<li class="toctree-l2"><a class="reference internal" href="params.html#data-structure-parameters">1.1. Data structure parameters</a><ul>
<li class="toctree-l3"><a class="reference internal" href="params.html#trial-specific-format">1.1.1. Trial-specific format</a></li>
<li class="toctree-l3"><a class="reference internal" href="params.html#trial-common-format">1.1.2. Trial-common format</a></li>
<li class="toctree-l3"><a class="reference internal" href="params.html#defaults">1.1.3. Defaults</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="params.html#initial-values-of-model-parameters">1.2. Initial values of model parameters</a><ul>
<li class="toctree-l3"><a class="reference internal" href="params.html#variational-parameters">1.2.1. Variational parameters</a></li>
<li class="toctree-l3"><a class="reference internal" href="params.html#embedding-parameters">1.2.2. Embedding parameters</a></li>
<li class="toctree-l3"><a class="reference internal" href="params.html#kernel-parameters">1.2.3. Kernel parameters</a></li>
<li class="toctree-l3"><a class="reference internal" href="params.html#inducing-points-locations-parameters">1.2.4. Inducing points locations parameters</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="params.html#optimisation-parameters">1.3. Optimisation parameters</a><ul>
<li class="toctree-l3"><a class="reference internal" href="params.html#id20">1.3.1. Defaults</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="references.html">2. References</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Code:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="svGPFA.html">1. svGPFA package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="svGPFA.html#subpackages">1.1. Subpackages</a><ul>
<li class="toctree-l3"><a class="reference internal" href="svGPFA.plot.html">1.1.1. svGPFA.plot package</a></li>
<li class="toctree-l3"><a class="reference internal" href="svGPFA.simulations.html">1.1.2. svGPFA.simulations package</a></li>
<li class="toctree-l3"><a class="reference internal" href="svGPFA.stats.html">1.1.3. svGPFA.stats package</a></li>
<li class="toctree-l3"><a class="reference internal" href="svGPFA.utils.html">1.1.4. svGPFA.utils package</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="svGPFA.html#module-svGPFA">1.2. Module contents</a></li>
</ul>
</li>
</ul>
</div>

  <button
      data-action="sidebar#close"
      title="Close navigation menu"
      class="text-4xl text-gray-800 p-4 bottom-0 hover:text-brand xl:hidden focus:text-brand self-center">
    <span class="sr-only">Close navigation menu</span>
    <svg aria-hidden="true" class="fill-current stroke-current h-6 w-6" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
  </button>
</nav>
</aside>

<main
  class="px-4 pt-14 xl:ml-fluid grid-area-main overflow-y-auto flex flex-col flex-1 h-full mx-0 md:mx-auto xl:mr-0"
><nav role="navigation"
     aria-label="breadcrumbs"
     class="print:hidden mt-12 text-sm text-gray-light">
  <a class="text-gray-light text-sm hover:text-gray-dark font-medium focus:text-gray-dark" href="index.html">svGPFA 0.0.1 documentation</a>
  <span class="mr-1">/</span><span aria-current="page"><span class="section-number">4. </span>Derivations</span>
</nav>
  <article role="main" class="flex-1">
    
  <section id="derivations">
<h1><span class="section-number">4. </span>Derivations<a class="headerlink" href="#derivations" title="Permalink to this heading"><span>#</span></a></h1>
<section id="gpfa-model">
<h2><span class="section-number">4.1. </span>GPFA model<a class="headerlink" href="#gpfa-model" title="Permalink to this heading"><span>#</span></a></h2>
<p>Equation <a class="reference internal" href="#equation-eq-gpfamodel">(4.1)</a> represents the GPFA model:</p>
<div class="math notranslate nohighlight" id="equation-eq-gpfamodel">
<span class="eqno">(4.1)<a class="headerlink" href="#equation-eq-gpfamodel" title="Permalink to this equation"><span>#</span></a></span>\[\begin{split}p(\{x_{kr}(\cdot)\}_{k=1,r=1}^{K,R}) &amp;= \prod_{r=1}^R\prod_{k=1}^Kp(x_{kr}(\cdot))\\
x_{kr}(\cdot) &amp;\sim \mathcal{GP}(\mu_k(\cdot),\kappa_k(\cdot,\cdot))&amp;&amp;{\text{for}\; k=1, \ldots, K\;\text{and}\;r=1,\ldots, R}\\
h_{nr}(\cdot) &amp;= \sum_{k=1}^Kc_{nk}x_{kr}(\cdot) + d_n&amp;&amp;{\text{for}\; n=1, \ldots, N\;\text{and}\;r=1,\ldots, R}\\
p(\{y_{nr}\}_{n=1,r=1}^{N,R}|\{h_{nr}(\cdot)\}_{n=1,r=1}^{N,R}) &amp;= \prod_{r=1}^R\prod_{n=1}^Np(y_{nr}|h_{nr}(\cdot))\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(x_{kr}(\cdot)\)</span> is the latent process <span class="math notranslate nohighlight">\(k\)</span> in trial <span class="math notranslate nohighlight">\(r\)</span>, <span class="math notranslate nohighlight">\(h_{nr}(\cdot)\)</span> is the embedding process for neuron <span class="math notranslate nohighlight">\(n\)</span> and trial <span class="math notranslate nohighlight">\(r\)</span> and <span class="math notranslate nohighlight">\(y_{nr}\)</span> is the activity of neuron <span class="math notranslate nohighlight">\(n\)</span> in trial <span class="math notranslate nohighlight">\(r\)</span>.</p>
<p>Notes:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>the first equation shows that the latent processes are independent,</p></li>
<li><p>the second equation shows that the latent processes share mean and covariance functions across trials. That is, for any <span class="math notranslate nohighlight">\(k\)</span>, the mean and covariance functions  of latents processes of different trials, <span class="math notranslate nohighlight">\(x_{kr}(\cdot), r=1,\ldots, R\)</span>, are the same (<span class="math notranslate nohighlight">\(\mu_k(\cdot)\)</span> and <span class="math notranslate nohighlight">\(\kappa_k(\cdot,\cdot)\)</span>),</p></li>
<li><p>the fourth equation shows that, given the embedding processes, the responses of different neurons are independent.</p></li>
</ol>
</div></blockquote>
</section>
<section id="gpfa-with-inducing-points-model">
<h2><span class="section-number">4.2. </span>GPFA with inducing points model<a class="headerlink" href="#gpfa-with-inducing-points-model" title="Permalink to this heading"><span>#</span></a></h2>
<p>To use the sparse variational framework for Gaussian processes,
<span id="id1">Duncker and Sahani [<a class="reference internal" href="references.html#id5" title="Lea Duncker and Maneesh Sahani. Temporal alignment and latent gaussian process factor inference in population spike trains. In Advances in Neural Information Processing Systems, 10445â€“10455. 2018.">DS18</a>]</span> augmented the GPFA model by introducing inducing
points <span class="math notranslate nohighlight">\(\mathbf{u}_{kr}\)</span> for each latent process <span class="math notranslate nohighlight">\(k\)</span> and trial
<span class="math notranslate nohighlight">\(r\)</span>. The inducing points <span class="math notranslate nohighlight">\(\mathbf{u}_{kr}\)</span> represent evaluations of
the latent process <span class="math notranslate nohighlight">\(x_{kr}(\cdot)\)</span> at locations
<span class="math notranslate nohighlight">\(\mathbf{z}_{kr}=\left[z_{kr}[0],\ldots,z_{kr}[M_{kr}-1]\right]\)</span>. A
joint prior over the latent process <span class="math notranslate nohighlight">\(x_{kr}(\cdot)\)</span> and its inducing
points <span class="math notranslate nohighlight">\(\mathbf{u}_{kr}\)</span> is given in Eq. <a class="reference internal" href="#equation-eq-gpfawithindpointsprior">(4.2)</a>.</p>
<div class="math notranslate nohighlight" id="equation-eq-gpfawithindpointsprior">
<span class="eqno">(4.2)<a class="headerlink" href="#equation-eq-gpfawithindpointsprior" title="Permalink to this equation"><span>#</span></a></span>\[\begin{split}p(\{x_{kr}(\cdot)\}_{k=1,r=1}^{K,R},\{\mathbf{u}_{kr}\}_{k=1,r=1}^{K,R}) &amp;= p(\{x_{kr}(\cdot)\}_{k=1,r=1}^{K,R}|\{\mathbf{u}_{kr}\}_{k=1,r=1}^{K,R})p(\{\mathbf{u}_{kr}\}_  {k=1,r=1}^{K,R})\\
p(\{x_{kr}(\cdot)\}_{k=1,r=1}^{K,R}|\{\mathbf{u}_{kr}\}_  {k=1,r=1}^{K,R}) &amp;= \prod_{k=1}^k\prod_{r=1}^{R}p(x_{kr}(\cdot)|\mathbf{u}_{kr})\\
p(\{\mathbf{u}_{kr}\}_{k=1,r=1}^{K,R})&amp;=\prod_{k=1}^k\prod_{r=1}^{R}p(\mathbf{u}_{kr})\\
p(\mathbf{u}_{kr})&amp;=\mathcal{N}(\mathbf{0},K^{kr}_{zz})\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(K_{zz}^{(kr)}[i,j]=\kappa_k(z_{kr}[i],z_{kr}[j])\)</span>.</p>
<p>We next derive the functional form of <span class="math notranslate nohighlight">\(p(x_{kr}(\cdot)|\mathbf{u}_{kr})\)</span>.</p>
<p>Define the random vector <span class="math notranslate nohighlight">\(\mathbf{x}_{kr}\)</span> as the random process
<span class="math notranslate nohighlight">\(x_{kr}(\cdot)\)</span> evaluated at times
<span class="math notranslate nohighlight">\(\mathbf{t}^{(r)}=\left\{t_1^{(r)},\ldots,t_M^{(r)}\right\}\)</span> (i.e.,
<span class="math notranslate nohighlight">\(\mathbf{x}_{kr}=[x_{kr}(t_1^{(r)}),\ldots,x_{kr}(t_M^{(r)})]^\intercal\)</span>).
Because the inducing points <span class="math notranslate nohighlight">\(\mathbf{u}_{kr}\)</span> are evaluations of the
latent process <span class="math notranslate nohighlight">\(x_{kr}(\cdot)\)</span> at <span class="math notranslate nohighlight">\(\mathbf{z}_{kr}\)</span>, then <span class="math notranslate nohighlight">\(\mathbf{x}_{kr}\)</span> and
<span class="math notranslate nohighlight">\(\mathbf{u}_{kr}\)</span> are jointly Gaussian:</p>
<div class="math notranslate nohighlight" id="equation-eq-prior">
<span class="eqno">(4.3)<a class="headerlink" href="#equation-eq-prior" title="Permalink to this equation"><span>#</span></a></span>\[\begin{split}p\left(\left[\begin{array}{c}
    \mathbf{u}_{kr}\\
    \mathbf{x}_{kr}
\end{array}\right]\right)
=\mathcal{N}\left(\left.\left[\begin{array}{c}
    \mathbf{u}_{kr}\\
    \mathbf{x}_{kr}
\end{array}\right]\right|\left[\begin{array}{c}
    \mathbf{0}\\
    \mathbf{0}
\end{array}\right],\left[\begin{array}{cc}
    K_\mathbf{zz}^{(kr)}&amp;K_\mathbf{zt}^{(kr)}\\
    K_\mathbf{tz}^{(kr)}&amp;K_\mathbf{tt}^{(r)}
\end{array}\right]\right)\end{split}\]</div>
<p>where
<span class="math notranslate nohighlight">\(K_\mathbf{tz}^{(kr)}[i,j]=\kappa_k(t^{(r)}_i,z_{kr}[j])\)</span>,
<span class="math notranslate nohighlight">\(K_\mathbf{zt}^{(kr)}[i,j]=\kappa_k(z_{kr}[i],t_j^{(r)})\)</span>
and
<span class="math notranslate nohighlight">\(K_\mathbf{tt}^{(r)}[i,j]=\kappa_k(t_i^{(r)},t_j^{(r)})\)</span>.</p>
<p>Now, applying the formula for the conditional pdf for jointly Normal random
vectors <span id="id2">[<a class="reference internal" href="references.html#id70" title="Christopher M Bishop. Pattern recognition and machine learning. Springer-Verlag New York, 2016.">Bis16</a>]</span>, Eq. 2116, to Eq. <a class="reference internal" href="#equation-eq-prior">(4.3)</a>, we obtain</p>
<div class="math notranslate nohighlight" id="equation-eq-latentconditionalindpointsvector">
<span class="eqno">(4.4)<a class="headerlink" href="#equation-eq-latentconditionalindpointsvector" title="Permalink to this equation"><span>#</span></a></span>\[p(\mathbf{x}_{kr}|\mathbf{u}_{kr})=\mathcal{N}\left(\mathbf{x}_{kr}\left|K_\mathbf{tz}^{(kr)}\left(K_{zz}^{(kr)}\right)^{-1}\mathbf{u}_{kr},\;K_\mathbf{tt}^{(r)}-K_\mathbf{tz}^{(kr)}\left(K_{zz}^{(kr)}\right)^{-1}K_\mathbf{zt}^{(kr)}\right.\right)\]</div>
<p>Because Eq. <a class="reference internal" href="#equation-eq-latentconditionalindpointsvector">(4.4)</a> is valid for any
<span class="math notranslate nohighlight">\(\mathbf{t}^{(r)}\)</span>, it follows that</p>
<div class="math notranslate nohighlight">
\[p(x_{kr}(\cdot)|\mathbf{u}_{kr})=\mathcal{GP}\left(\tilde{\mu}_{kr}(\cdot), \tilde{\kappa}_{kr}(\cdot,\cdot\right))\]</div>
<p>with</p>
<div class="math notranslate nohighlight">
\[\begin{split}\tilde{\mu}_{kr}(t)&amp;=\kappa_k(t,\mathbf{z}_{kr})\left(K_{zz}^{(kr)}\right)^{-1}\mathbf{u}_{kr},\\
\tilde{\kappa}_k(t,t')&amp;=\kappa_k(t,t')-\kappa_k(t,\mathbf{z}_{kr})\left(K_{zz}^{(kr)}\right)^{-1}\kappa_k(\mathbf{z}_{kr},t')\end{split}\]</div>
<p>which is Eq. 3 in <span id="id3">Duncker and Sahani [<a class="reference internal" href="references.html#id5" title="Lea Duncker and Maneesh Sahani. Temporal alignment and latent gaussian process factor inference in population spike trains. In Advances in Neural Information Processing Systems, 10445â€“10455. 2018.">DS18</a>]</span>.</p>
</section>
<section id="svgpfa-variational-lower-bound">
<h2><span class="section-number">4.3. </span>svGPFA variational lower bound<a class="headerlink" href="#svgpfa-variational-lower-bound" title="Permalink to this heading"><span>#</span></a></h2>
<p><a class="reference internal" href="#thmvariationallowerbound"><span class="std std-numref">Theorem 4.1</span></a> proves Eq. 4 in
<span id="id4">Duncker and Sahani [<a class="reference internal" href="references.html#id5" title="Lea Duncker and Maneesh Sahani. Temporal alignment and latent gaussian process factor inference in population spike trains. In Advances in Neural Information Processing Systems, 10445â€“10455. 2018.">DS18</a>]</span>.</p>
<div class="proof proof-type-theorem" id="id9">
<span id="thmvariationallowerbound"></span>
    <div class="proof-title">
        <span class="proof-type">Theorem 4.1</span>
        
            <span class="proof-title-name">(svGPFA Variational Lower Bound)</span>
        
    </div><div class="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\mathcal{Y}=\{y_{nr}\}_{n=1,r=1}^{N,R}\)</span> then</p>
<div class="math notranslate nohighlight" id="equation-eq-variationallowerbound">
<span class="eqno">(4.5)<a class="headerlink" href="#equation-eq-variationallowerbound" title="Permalink to this equation"><span>#</span></a></span>\[\log p(\mathcal{Y})\ge\sum_{n=1}^N\sum_{r=1}^R\mathbb{E}_{q\left(h_{nr}(\cdot)\right)}\left\{\log p(y_{nr}|h_{nr}(\cdot))\right\}-\sum_{r=1}^R\sum_{k=1}^KKL(q(\mathbf{u}_{kr})||p(\mathbf{u}_{kr}))\]</div>
</div></div><div class="proof proof-type-proof">

    <div class="proof-title">
        <span class="proof-type">Proof </span>
        
    </div><div class="proof-content">
<p>We begin with the joint-data likelihood of the full model, given in Eq.1 of
the supplementary material in <span id="id5">Duncker and Sahani [<a class="reference internal" href="references.html#id5" title="Lea Duncker and Maneesh Sahani. Temporal alignment and latent gaussian process factor inference in population spike trains. In Advances in Neural Information Processing Systems, 10445â€“10455. 2018.">DS18</a>]</span></p>
<div class="math notranslate nohighlight" id="equation-eq-jointdatalikelihood">
<span class="eqno">(4.6)<a class="headerlink" href="#equation-eq-jointdatalikelihood" title="Permalink to this equation"><span>#</span></a></span>\[p\left(\mathcal{Y},\{x_{kr}(\cdot)\}_{k=1,r=1}^{K,R},\{\mathbf{u}_{kr}\}_{k=1,r=1}^{K,R}\right)=p\left(\mathcal{Y}|\{x_{kr}(\cdot)\}_{k=1,r=1}^{K,R}\right)\prod_{k=1}^K\prod_{r=1}^Rp(x_{kr}(\cdot)|\mathbf{u}_{kr})p(\mathbf{u}_{kr})\]</div>
<p>For notational clarity, from now on we omit the bounds of the <span class="math notranslate nohighlight">\(k\)</span> and <span class="math notranslate nohighlight">\(r\)</span> indices. From <a class="reference internal" href="#corollaryvariationalinequality"><span class="std std-numref">Corollary 4.3</span></a> by taking <span class="math notranslate nohighlight">\(x=\mathcal{Y}\)</span> and <span class="math notranslate nohighlight">\(z=\left(\{x_{kr}(\cdot)\},\{\mathbf{u}_{kr}\}\right)\)</span>, we obtain</p>
<div class="math notranslate nohighlight" id="equation-eq-vlbproof1">
<span class="eqno">(4.7)<a class="headerlink" href="#equation-eq-vlbproof1" title="Permalink to this equation"><span>#</span></a></span>\[\log p\left(\mathcal{Y}\right)\ge\int\int q\left(\{x_{kr}(\cdot)\},\{\mathbf{u}_{kr}\}\right)\log\frac{p\left(\mathcal{Y},\{x_{kr}(\cdot)\},\{\mathbf{u}_{kr}\}\right)}{q\left(\{x_{kr}(\cdot)\},\{\mathbf{u}_{kr}\}\right)}d\{x_{kr}(\cdot)\}d\{\mathbf{u}_{kr}\}\]</div>
<p>Choosing</p>
<div class="math notranslate nohighlight">
\[q\left(\{x_{kr}(\cdot)\},\{\mathbf{u}_{kr}\}\right)=\prod_{r=1}^R\prod_{k=1}^Kp(x_{kr}(\cdot)|\mathbf{u}_{kr})q(\mathbf{u}_{kr})\]</div>
<p>and using Eq. <a class="reference internal" href="#equation-eq-jointdatalikelihood">(4.6)</a> we can rewrite Eq. <a class="reference internal" href="#equation-eq-vlbproof1">(4.7)</a> as</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}\log p\left(\mathcal{Y}\right)\ge&amp;\int\int q\left(\{x_{kr}(\cdot)\},\{\mathbf{u}_{kr}\}\right)\left(\log p\left(\mathcal{Y}|\{x_{kr}(\cdot)\}\right)-\sum_{r=1}^R\sum_{k=1}^K\log\frac{q\left(\mathbf{u}_{kr}\right)}{p\left(\mathbf{u}_{kr}\right)}\right)d\{x_{kr}(\cdot)\}d\{\mathbf{u}_{kr}\}\nonumber\\
                                 =&amp;\int\int q\left(\{x_{kr}(\cdot)\},\{\mathbf{u}_{kr}\}\right)\log p\left(\mathcal{Y}|\{x_{kr}(\cdot)\}\right)d\{x_{kr}(\cdot)\}d\{\mathbf{u}_{kr}\}-\nonumber\\
                                  &amp;\int\int q\left(\{x_{kr}(\cdot)\},\{\mathbf{u}_{kr}\}\right)\sum_{r=1}^R\sum_{k=1}^K\log\frac{q\left(\mathbf{u}_{kr}\right)}{p\left(\mathbf{u}_{kr}\right)}d\{x_{kr}(\cdot)\}d\{\mathbf{u}_{kr}\}\nonumber\\
                                 =&amp;\int q\left(\{x_{kr}(\cdot)\}\right)\log p\left(\mathcal{Y}|\{x_{kr}(\cdot)\}\right)d\{x_{kr}(\cdot)\}-\sum_{r=1}^R\sum_{k=1}^K\int q\left(\mathbf{u}_{kr}\right)\log\frac{q\left(\mathbf{u}_{kr}\right)}{p\left(\mathbf{u}_{kr}\right)}d\mathbf{u}_{kr}\nonumber\\
                                 =&amp;\;\mathbb{E}_{q\left(\{x_{kr}(\cdot)\}\right)}\left\{\log p\left(\mathcal{Y}|\{x_{kr}(\cdot)\}\right)\right\}-\sum_{r=1}^R\sum_{k=1}^K\text{KL}\left(q\left(\mathbf{u}_{kr}\right)||p\left(\mathbf{u}_{kr}\right)\right)\\
                                 =&amp;\;\mathbb{E}_{q\left(\{h_{nr}(\cdot)\}\right)}\left\{\log p\left(\mathcal{Y}|\{h_{nr}(\cdot)\}\right)\right\}-\sum_{r=1}^R\sum_{k=1}^K\text{KL}\left(q\left(\mathbf{u}_{kr}\right)||p\left(\mathbf{u}_{kr}\right)\right)\\
                                 =&amp;\;\mathbb{E}_{q\left(\{h_{nr}(\cdot)\}\right)}\left\{\sum_{n=1}^N\sum_{r=1}^R\log p\left(y_{nr}|h_{nr}(\cdot)\right)\right\}-\sum_{r=1}^R\sum_{k=1}^K\text{KL}\left(q\left(\mathbf{u}_{kr}\right)||p\left(\mathbf{u}_{kr}\right)\right)\\
                                 =&amp;\;\sum_{n=1}^N\sum_{r=1}^R\mathbb{E}_{q\left(h_{nr}(\cdot)\right)}\left\{\log p\left(y_{nr}|h_{nr}(\cdot)\right)\right\}-\sum_{r=1}^R\sum_{k=1}^K\text{KL}\left(q\left(\mathbf{u}_{kr}\right)||p\left(\mathbf{u}_{kr}\right)\right)\nonumber\end{split}\]</div>
</div></blockquote>
<dl class="simple">
<dt>Notes:</dt><dd><ol class="arabic simple">
<li><p>the derivation of the equation in the sixth line from that in the fifth one is subtle. It assumes that there exists a measurable and injective change of variables function <span class="math notranslate nohighlight">\(f(\{x_{kr}(\cdot)\})=\{h_{nr}(\cdot)\}\)</span>.</p></li>
<li><p>the equation in the seventh line follows from that in the sixth one using the last line in Eq. <a class="reference internal" href="#equation-eq-gpfamodel">(4.1)</a>.</p></li>
</ol>
</dd>
</dl>
</div></div><hr class="docutils" />
<div class="proof proof-type-lemma" id="id10">
<span id="lemmavariationalequality"></span>
    <div class="proof-title">
        <span class="proof-type">Lemma 4.2</span>
        
            <span class="proof-title-name">(Variational Equality)</span>
        
    </div><div class="proof-content">
<div class="math notranslate nohighlight">
\[\log p(x) = \mathbb{E}_{q(z)}\left\{\log\frac{p(x,z)}{q(z)}\right\}+\text{KL}\left\{q(z)||p(z|x)\right\}\]</div>
</div></div><div class="proof proof-type-proof">

    <div class="proof-title">
        <span class="proof-type">Proof </span>
        
    </div><div class="proof-content">
<div class="math notranslate nohighlight">
\[\begin{split}p(x)&amp;=\frac{p(x,z)}{p(z|x)}=\frac{p(x,z)}{q(z)}\frac{q(z)}{p(z|x)}\\
\log p(x)&amp;=\log\frac{p(x,z)}{q(z)}+\log\frac{q(z)}{p(z|x)}\\
\log p(x)&amp;=\mathbb{E}_{q(z)}\left\{\log\frac{p(x,z)}{q(z)}\right\}+\mathbb{E}_{q(z)}\left\{\log\frac{q(z)}{p(z|x)}\right\}\\
\log p(x)&amp;=\mathbb{E}_{q(z)}\left\{\log\frac{p(x,z)}{q(z)}\right\}+\text{KL}\left\{q(z)||p(z|x)\right\}\end{split}\]</div>
<dl class="simple">
<dt>Notes:</dt><dd><ol class="arabic simple">
<li><p>the first equation uses Bayes rule,</p></li>
<li><p>the third equation applies the expected value to both sides of the second equation,</p></li>
<li><p>the last equation uses the definition of the KL divergence.</p></li>
</ol>
</dd>
</dl>
</div></div><hr class="docutils" />
<div class="proof proof-type-corollary" id="id11">
<span id="corollaryvariationalinequality"></span>
    <div class="proof-title">
        <span class="proof-type">Corollary 4.3</span>
        
            <span class="proof-title-name">(Variational Inequality)</span>
        
    </div><div class="proof-content">
<div class="math notranslate nohighlight" id="equation-eq-variationalinequality">
<span class="eqno">(4.8)<a class="headerlink" href="#equation-eq-variationalinequality" title="Permalink to this equation"><span>#</span></a></span>\[\log p(x) \ge \mathbb{E}_{q(z)}\left\{\log\frac{p(x,z)}{q(z)}\right\}\]</div>
<p>with equality if and only if <span class="math notranslate nohighlight">\(q(z)=p(z|x)\)</span>.</p>
</div></div><div class="proof proof-type-proof">

    <div class="proof-title">
        <span class="proof-type">Proof </span>
        
    </div><div class="proof-content">
<p>Equation <a class="reference internal" href="#equation-eq-variationalinequality">(4.8)</a> follows from
<a class="reference internal" href="#lemmavariationalequality"><span class="std std-numref">Lemma 4.2</span></a> by the
fact that the KL divergence between two
distributions is greater or equal than zero, with equality if and only if
the distributions are equal (Information inequality,
<span id="id6">Cover and Thomas [<a class="reference internal" href="references.html#id78" title="Thomas M. Cover and Joy A. Thomas. Elements of information theory. John Wiley &amp; Sons, 1991.">CT91</a>]</span>, Theorem 2.6.3).</p>
</div></div></section>
<hr class="docutils" />
<section id="variational-distribution-of-h-nr-cdot">
<h2><span class="section-number">4.4. </span>Variational distribution of <span class="math notranslate nohighlight">\(h_{nr}(\cdot)\)</span><a class="headerlink" href="#variational-distribution-of-h-nr-cdot" title="Permalink to this heading"><span>#</span></a></h2>
<p>For the calculation of the lower bound in the right-hand side of
Eq. <a class="reference internal" href="#equation-eq-variationallowerbound">(4.5)</a>, below we derive the distribution
<span class="math notranslate nohighlight">\(q(h_{nr}(\cdot))\)</span>.</p>
<p>We first deduce the distribution <span class="math notranslate nohighlight">\(q(x_{xr}(\cdot))\)</span>. Note, from
Eq. <a class="reference internal" href="#equation-eq-gpfawithindpointsprior">(4.2)</a>, that for any <span class="math notranslate nohighlight">\(P\in\mathbb{N}\)</span> and for any
<span class="math notranslate nohighlight">\(\mathbf{t}=(t_1,\ldots,t_P)\in\mathbb{R}^P\)</span> the approximate variational
posterior of the random vectors
<span class="math notranslate nohighlight">\(\mathbf{x}_{kr}=(x_{kr}(t_1),\ldots,x_{kr}(t_P))\)</span> and <span class="math notranslate nohighlight">\(\mathbf{u}_{kr}\)</span> is
jointly Gaussian</p>
<div class="math notranslate nohighlight">
\[\begin{split}q(\mathbf{x}_{kr},\mathbf{u}_{kr})&amp;=p(\mathbf{x}_{kr}|\mathbf{u}_{kr})q(\mathbf{u}_{kr})\\
                                  &amp;=\mathcal{N}\left(\mathbf{x}_{kr}|K_{tz}^{kr}(K_{zz}^{kr})^{-1}\mathbf{u}_{kr},\;K_{tt}^k-K_{tz}^{kr}(K_{zz}^{kr})^{-1}K_{zt}^{kr}\right)\mathcal{N}(\mathbf{u}_{kr}|\mathbf{m}_{kr},\;S_{kr})\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(K_{tt}\)</span>, <span class="math notranslate nohighlight">\(K_{tz}\)</span>, <span class="math notranslate nohighlight">\(K_{zt}\)</span>, and <span class="math notranslate nohighlight">\(K_{zz}\)</span> are covariance matrices obtained by evalating of <span class="math notranslate nohighlight">\(\kappa_k(t,t')\)</span>, <span class="math notranslate nohighlight">\(\kappa_k(t,z)\)</span>, <span class="math notranslate nohighlight">\(\kappa_k(z,t)\)</span>, and <span class="math notranslate nohighlight">\(\kappa_k(z,z')\)</span>, respectively, at <span class="math notranslate nohighlight">\(t,t'\in \{t_1,\ldots t_P\}\)</span> and <span class="math notranslate nohighlight">\(z,z'\in \{\mathbf{z}_{kr}[1],\ldots,\mathbf{z}_{kr}[M_{kr}]\}\)</span>. Next, using the expression for the marginal of a joint Gaussian distribution (e.g., Eq.~2.115 in <span id="id7">Bishop [<a class="reference internal" href="references.html#id70" title="Christopher M Bishop. Pattern recognition and machine learning. Springer-Verlag New York, 2016.">Bis16</a>]</span>) we obtain</p>
<div class="math notranslate nohighlight" id="equation-eq-qxrandomvec">
<span class="eqno">(4.9)<a class="headerlink" href="#equation-eq-qxrandomvec" title="Permalink to this equation"><span>#</span></a></span>\[q(\mathbf{x}_{kr})=\mathcal{N}\left(\mathbf{x}_{kr}|K_{tz}^{kr}(K_{zz}^{kr})^{-1}\mathbf{m}_{kr},\;K_{tt}^k+K_{tz}^{kr}\left((K_{zz}^{kr})^{-1}S_{kr}(K_{zz}^{kr})^{-1}-(K_{zz}^{kr})^{-1  }\right)K_{zt}^{kr}\right)\]</div>
<p>Because Eq. <a class="reference internal" href="#equation-eq-qxrandomvec">(4.9)</a> holds for any <span class="math notranslate nohighlight">\(P\in\mathbb{N}\)</span> and for any <span class="math notranslate nohighlight">\(t_1,\ldots,t_P)\in\mathbb{R}^P\)</span> then</p>
<div class="math notranslate nohighlight" id="equation-eq-qx">
<span class="eqno">(4.10)<a class="headerlink" href="#equation-eq-qx" title="Permalink to this equation"><span>#</span></a></span>\[\begin{split}q(x_{kr}(\cdot))&amp;=\mathcal{GP}\left(\breve\mu_{kr}(\cdot),\breve\kappa_{kr}(\cdot,\cdot)\right)\\
\breve\mu_{kr}(t)&amp;=\kappa_k(t,z_{kr})(K_{zz}^{kr})^{-1}\mathbf{m}_{kr},\\
\breve\kappa_{kr}(t,t')&amp;=\kappa_k(t,t')+\kappa_k(t,z_{kr})\left((K_{zz}^{kr})^{-1}S_{kr}(K_{zz}^{kr})^{-1}-(K_{zz}^{kr})^{-1}\right)\kappa_k(z_{kr},t')\end{split}\]</div>
<p>Finally, because affine trasformations of Gaussians are Gaussians,
<span class="math notranslate nohighlight">\(h_{nr}(\cdot)\)</span> is an affine transformation of <span class="math notranslate nohighlight">\(\{x_{kr}(\cdot)\}\)</span> (which are
Gaussians, Eq. <a class="reference internal" href="#equation-eq-qx">(4.10)</a>), then the approximate posterior of <span class="math notranslate nohighlight">\(h_{nr}(\cdot)\)</span>
is the Gaussian process in Eq. <a class="reference internal" href="#equation-eq-qh">(4.11)</a>.</p>
<div class="math notranslate nohighlight" id="equation-eq-qh">
<span class="eqno">(4.11)<a class="headerlink" href="#equation-eq-qh" title="Permalink to this equation"><span>#</span></a></span>\[\begin{split}q(h_{nr}(\cdot))&amp;=\mathcal{GP}\left(\tilde\mu_{nr}(\cdot),\tilde\kappa_{nr}(\cdot,\cdot)\right)\\
\tilde\mu_{nr}(t)&amp;=\sum_{k=1}^Kc_{nk}\breve\mu_{kr}(t)+d_n\\
\tilde\kappa_{nr}(t,t')&amp;=\sum_{k=1}^Kc_{nk}^2\breve\kappa_{kr}(t,t')\end{split}\]</div>
<p>which is Eq. 5 in <span id="id8">Duncker and Sahani [<a class="reference internal" href="references.html#id5" title="Lea Duncker and Maneesh Sahani. Temporal alignment and latent gaussian process factor inference in population spike trains. In Advances in Neural Information Processing Systems, 10445â€“10455. 2018.">DS18</a>]</span>.</p>
</section>
</section>


  </article>

  <footer>
    
      <div class="mt-20 mb-4 text-sm text-gray-700 print:mt-4">&copy; 2019, Lea Duncker and Maneesh Sahani&nbsp;Made with <a href="https://www.sphinx-doc.org">Sphinx 6.1.3</a></div>
    
  </footer></main>


      <button
  data-search-target="snackbar"
  data-action="search#hideSnackbar"
  class="fixed bottom-0 right-0 z-20 opacity-0 p-4 m-4 tracking-wide bg-gray-900 text-gray-100 transition transform transform-gpu duration-500 translate-y-full"
>
  Clear highlights
</button><div
  data-sidebar-target="screen"
  data-action="click->sidebar#close"
  class="fixed hidden inset-0 bg-black bg-opacity-50">
</div>
    </div>
    
      
<script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
<script src="_static/doctools.js"></script>
<script src="_static/sphinx_highlight.js"></script>
<script src="_static/proof.js"></script>
<script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script src="_static/theme.b62e1ded0c8c099a2d47.js"></script>
    
  </body>
</html>